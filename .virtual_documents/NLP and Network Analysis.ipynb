


import pandas as pd
import numpy as np
import spacy
import os


# Load the Text file 
with open('20th_century_events.txt', 'r', encoding='utf-8', errors='ignore') as file:
    data = file.read()


print(data[:1000])





data_clean = data.replace('\n', ' ')





import re
# This finds all [1], [2], etc., and replaces them with nothing
data_clean = re.sub(r'\[\d+\]', '', data_clean)





# Standardize names so they match lookup list
data_clean = data_clean.replace('USA', 'United States')





with open('twentieth_century_CLEAN.txt', 'w', encoding='utf-8') as f:
    f.write(data_clean)








nlp = spacy.load("en_core_web_sm")
ner_doc = nlp(data_clean)








df_sentences = []

# Loop through every sentence identified by spaCy
for sent in ner_doc.sents:
    # Get a list of the text for every entity in that sentence
    entity_list = [ent.text for ent in sent.ents]
    # Store the sentence and its found entities in a dictionary
    df_sentences.append({"sentence": sent, "entities": entity_list})

# Turn the list into a structured table (DataFrame)
df_sentences = pd.DataFrame(df_sentences)








# Read text file and turn it into a list of names
with open('countries_lookup.txt', 'r') as f:
    countries = [line.strip() for line in f]





# This function keeps an entity only if it is in your country list
def filter_entities(ent_list, lookup_list):
    return [ent for ent in ent_list if ent in lookup_list]

# Create a new column containing ONLY the matched countries
df_sentences['character_entities'] = df_sentences['entities'].apply(lambda x: filter_entities(x, countries))

# Remove any sentences that now have zero countries left
df_sentences_filtered = df_sentences[df_sentences['character_entities'].map(len) > 0]








relationships = []

# Iterate through the filtered sentences
for i in range(len(df_sentences_filtered)):
    # Look at a window of the next 5 sentences
    end_i = min(i + 5, len(df_sentences_filtered))
    # Combine all country entities found in this 5-sentence block
    char_list = sum((df_sentences_filtered.iloc[i:end_i].character_entities), [])

    # Remove duplicates appearing right next to each other
    char_unique = [char_list[j] for j in range(len(char_list)) 
                   if (j == 0) or char_list[j] != char_list[j-1]]

    # If more than one country is in the window, they have a relationship
    if len(char_unique) > 1:
        for idx, a in enumerate(char_unique[:-1]):
            b = char_unique[idx + 1]
            relationships.append({"source": a, "target": b})

# Create the final relationship table
relationship_df = pd.DataFrame(relationships)








# Count how many times each pair appeared
relationship_df = pd.DataFrame(relationships)
relationship_df = relationship_df.groupby(["source", "target"], sort=False, as_index=False).sum()
# Save to your path
relationship_df.to_csv('country_relationships.csv', index=False)






