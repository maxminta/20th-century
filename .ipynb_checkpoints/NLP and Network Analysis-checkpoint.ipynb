{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af2c515b-4e71-448a-9c54-8661a7c1777a",
   "metadata": {},
   "source": [
    "# Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40bc3987-6e9e-4315-b24f-7f55e6ae889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91c9b1f0-794f-4116-8d72-8948dcefd49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Text file \n",
    "with open('20th_century_events.txt', 'r', encoding='utf-8', errors='ignore') as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dc3d20a-463f-4931-89bb-c8a46a03d20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Key events of the 20th century - Wikipedia\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Jump to content\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Main menu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Main menu\n",
      "move to sidebar\n",
      "hide\n",
      "\n",
      "\n",
      "\n",
      "\t\tNavigation\n",
      "\t\n",
      "\n",
      "\n",
      "Main pageContentsCurrent eventsRandom articleAbout WikipediaContact us\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tContribute\n",
      "\t\n",
      "\n",
      "\n",
      "HelpLearn to editCommunity portalRecent changesUpload fileSpecial pages\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Appearance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Donate\n",
      "\n",
      "Create account\n",
      "\n",
      "Log in\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Personal tools\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Donate Create account Log in\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "move to sidebar\n",
      "hide\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(Top)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "Historic events in the 20th century\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Toggle Historic events in the 20th century subsection\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.1\n",
      "World at the beginning of the century\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.1.1\n",
      "\"The war to end all wars\": World War I (1914‚Äì1918)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.2\n",
      "Spanish flu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.2.1\n",
      "Russian Revolution and communism\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.3\n",
      "Between the wars\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.3.1\n",
      "Economic depression\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.3.2\n",
      "The rise of dictatorship\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.4\n",
      "Global war: World War II (1\n"
     ]
    }
   ],
   "source": [
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1933809-5c3c-4fc5-bed7-9b6520382999",
   "metadata": {},
   "source": [
    "### Cleaning New Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dd46c41-1bb2-4b72-8c3c-b496cc9e3087",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778b924e-1c22-4281-b697-13b9980bf976",
   "metadata": {},
   "source": [
    "### Cleaning Citations (The Regex Tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff46b00d-9791-4bbc-b7de-98946248bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# This finds all [1], [2], etc., and replaces them with nothing\n",
    "data_clean = re.sub(r'\\[\\d+\\]', '', data_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36919bba-a6f0-4008-98f9-f3577ad47712",
   "metadata": {},
   "source": [
    "### Fixing Country Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7809fe68-35aa-4611-a7a8-1497d28ec633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize names so they match lookup list\n",
    "data_clean = data_clean.replace('USA', 'United States')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7269dd4d-1e0c-4262-a3c4-67f45ad1bc71",
   "metadata": {},
   "source": [
    "### Saving the \"Clean\" Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cd77a20-9a99-4f89-9c34-65ad542d7ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('twentieth_century_CLEAN.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(data_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf66990-1220-43b1-a78f-dd4b2e912f7b",
   "metadata": {},
   "source": [
    "## Data Cleaning & Wrangling Observations\n",
    "\n",
    "**Project: Twentieth-Century Key Events NER Analysis**\n",
    "During the initial evaluation of the scraped twentieth-century text file, several \"dirty\" data issues were identified that could interfere with the Named Entity Recognition (NER) algorithm. Below are the specific observations and the corrective actions taken.\n",
    "\n",
    "**üîç Initial Observations**\n",
    "Whitespace and Line Breaks: The raw text contained numerous \\n (newline) characters. In NLP, these can prematurely break a sentence, causing the algorithm to miss relationships between countries that span across a line break.\n",
    "\n",
    "**Citation Markers**: Because the data was scraped, it contained Wikipedia-style citations (e.g., [1], [15]). These are problematic because the NER algorithm might try to process them as part of a word or a \"Work of Art\" entity.\n",
    "\n",
    "**Special Characters**: Unexpected symbols and non-standard spacing were present, which can confuse the Tokenization step of the NLP process.\n",
    "\n",
    "**Entity Inconsistency**: Some countries were referred to by multiple names (e.g., \"U.S.A.\" vs \"United States\"). These must be standardized to match the countries_lookup.txt list exactly for the filtering step to work correctly.\n",
    "\n",
    "**üõ†Ô∏è Cleaning Steps Taken**\n",
    "Removing Line Breaks: I used the .replace('\\n', ' ') method to turn all hard returns into single spaces, creating a continuous flow of text for better sentence segmentation.\n",
    "\n",
    "**Removing Citations**: I utilized the re (Regular Expression) library to identify and remove any digits inside square brackets.\n",
    "\n",
    "Pattern used: r'\\[\\d+\\]'.\n",
    "\n",
    "**Standardizing Country Names**: I performed a manual check against my country list and used string replacement to ensure that country names in the text (Source) match the names in my lookup dictionary (Reference).\n",
    "\n",
    "**Encoding Check**: The file was saved using UTF-8 encoding to ensure that any special characters or accented names (like \"C√¥te d'Ivoire\") are preserved correctly without turning into garbage text.\n",
    "\n",
    "**‚úÖ Final Result**\n",
    "The cleaned text is now a \"Sequence of Tokens\" ready to be converted into a spaCy Doc object. By removing the \"noise\" (citations and line breaks), the Dependency Parsing step can now accurately look at the tokens in their specific context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c150f7f8-2a3b-41a8-9c94-ccbfd8ebccc3",
   "metadata": {},
   "source": [
    "### NER Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad4e8869-2e2a-4f70-ad3a-f956235e682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ner_doc = nlp(data_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8771d4-9544-49a9-ba99-c14cf1ab8f8c",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "**üõ†Ô∏è Creating the NER Object**\n",
    "\n",
    "- The Goal: To transform raw text into a \"smart\" document that understands grammar and entities.\n",
    "\n",
    "- The Process: I loaded the spaCy English language module (en_core_web_sm) and passed my cleaned text through it to create a Doc object.\n",
    "\n",
    "- Result: Every word is now analyzed. The AI can now distinguish between a country (labeled as GPE), a person, or a date."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b2a819-82ca-433a-be54-7cb0cfb442dd",
   "metadata": {},
   "source": [
    "### Splitting Sentence Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "807eb3cd-aacb-4c7e-9a33-4ba4ac491fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences = []\n",
    "\n",
    "# Loop through every sentence identified by spaCy\n",
    "for sent in ner_doc.sents:\n",
    "    # Get a list of the text for every entity in that sentence\n",
    "    entity_list = [ent.text for ent in sent.ents]\n",
    "    # Store the sentence and its found entities in a dictionary\n",
    "    df_sentences.append({\"sentence\": sent, \"entities\": entity_list})\n",
    "\n",
    "# Turn the list into a structured table (DataFrame)\n",
    "df_sentences = pd.DataFrame(df_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5884c0e7-16d3-47a6-9abf-cc4bf85c1269",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "In this step, you take the \"smart\" ner_doc created by spaCy and break it down so that you can see which entities appear together in specific sentences.\n",
    "\n",
    "**‚úÇÔ∏è Splitting Sentence Entities**\n",
    "- The Goal: To break the text down into smaller, manageable pieces (sentences).\n",
    "\n",
    "- The Process: I used a loop to iterate through every sentence in the book.\n",
    "\n",
    "- Technical Output: I created a table where each row contains one sentence and a list of all the \"entities\" (important names) found within it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd545156-1c9b-4bf2-9a82-5f69cd774536",
   "metadata": {},
   "source": [
    "### Load lookup list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f6d8f64-7527-4ff2-bd19-dedbf76ed21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text file and turn it into a list of names\n",
    "with open('countries_lookup.txt', 'r') as f:\n",
    "    countries = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf07f20b-97e3-44c5-ab49-b586fefe0843",
   "metadata": {},
   "source": [
    "### Filter the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "961b086e-4f90-4c5a-94c6-8949520a7824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function keeps an entity only if it is in your country list\n",
    "def filter_entities(ent_list, lookup_list):\n",
    "    return [ent for ent in ent_list if ent in lookup_list]\n",
    "\n",
    "# Create a new column containing ONLY the matched countries\n",
    "df_sentences['character_entities'] = df_sentences['entities'].apply(lambda x: filter_entities(x, countries))\n",
    "\n",
    "# Remove any sentences that now have zero countries left\n",
    "df_sentences_filtered = df_sentences[df_sentences['character_entities'].map(len) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e62ccf1-8a64-4e85-a627-82968070eef2",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "**üîç Filtering with the Country Lookup**\n",
    "I filtered out \"noise\". If a sentence mentioned \"Winston Churchill\" and \"France,\" this step deletes \"Winston Churchill\" because he is a person, leaving only \"France\".\n",
    "\n",
    "- The Goal: To remove all \"noise\" and focus only on the countries of interest.\n",
    "\n",
    "- The Process: I loaded my countries_lookup.txt file and compared it against the entities found by the AI.\n",
    "\n",
    "- Result: Any entity not on my specific list (like names of people or dates) was discarded, leaving a clean dataset of country mentions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28590d26-c2f5-4bb8-ad20-6ee0b6d42877",
   "metadata": {},
   "source": [
    "### Creating Relationships (The Sliding Window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2562bd39-bba2-4957-b97d-d07c5e20d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships = []\n",
    "\n",
    "# Iterate through the filtered sentences\n",
    "for i in range(len(df_sentences_filtered)):\n",
    "    # Look at a window of the next 5 sentences\n",
    "    end_i = min(i + 5, len(df_sentences_filtered))\n",
    "    # Combine all country entities found in this 5-sentence block\n",
    "    char_list = sum((df_sentences_filtered.iloc[i:end_i].character_entities), [])\n",
    "\n",
    "    # Remove duplicates appearing right next to each other\n",
    "    char_unique = [char_list[j] for j in range(len(char_list)) \n",
    "                   if (j == 0) or char_list[j] != char_list[j-1]]\n",
    "\n",
    "    # If more than one country is in the window, they have a relationship\n",
    "    if len(char_unique) > 1:\n",
    "        for idx, a in enumerate(char_unique[:-1]):\n",
    "            b = char_unique[idx + 1]\n",
    "            relationships.append({\"source\": a, \"target\": b})\n",
    "\n",
    "# Create the final relationship table\n",
    "relationship_df = pd.DataFrame(relationships)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3893e8c9-41c7-4c0b-b8cf-66e0706af6a8",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "**ü§ù Creating Relationships (Sliding Window)**\n",
    "- The Goal: To identify which countries interacted with each other based on how close they appear in the text.\n",
    "\n",
    "- The Logic: I implemented a Sliding Window of 5 sentences. If two different countries appear within this 5-sentence span, the code records a \"relationship\" between them.\n",
    "\n",
    "- Final Calculation: I grouped these pairs together to count the total number of interactions, which tells us the \"strength\" of the connection between specific countries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97595528-8a9c-4249-a66e-9efefe2b69ed",
   "metadata": {},
   "source": [
    "### Exporting The Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "305126de-6fd9-4a13-b3f5-f0565499098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many times each pair appeared\n",
    "relationship_df = pd.DataFrame(relationships)\n",
    "relationship_df = relationship_df.groupby([\"source\", \"target\"], sort=False, as_index=False).sum()\n",
    "# Save to your path\n",
    "relationship_df.to_csv('country_relationships.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ae097b-5c11-49c6-9c11-f7de39606e07",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "**üíæFinalizing and Exporting the Data**\n",
    "\n",
    "- This final step transforms the raw list of interactions into a structured file ready for network visualization.\n",
    "\n",
    "- Aggregating Relationships: I used the .groupby() function to sum up every instance where two countries appeared together, creating a \"weight\" for each connection.\n",
    "\n",
    "- The Goal: This step converts hundreds of individual mentions into a summary table that shows exactly how many times each country pair interacted throughout the text.\n",
    "\n",
    "- Exporting to CSV: The final table was saved as a CSV file to the project path: C:\\Users\\ANITA BOADU\\Twentieth Century Project\\20th_century.\n",
    "\n",
    "- Ready for Visualization: This file now serves as the \"source of truth\" for building the character network graph in the next exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685b51f7-7bba-4423-aac5-e761b89c4be4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "history_final",
   "language": "python",
   "name": "history_final"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
